{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nayvti380/Transfer_Learning/blob/main/CatsxDogs_Transfer_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "45ticwyzsuqg"
      },
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning - Problema cats x dogs\n",
        "\n",
        "Implementação de rede convolucional usando transfer learning para diferenciação das categorias gato e cachorro.\n",
        "\n",
        " O banco de dados original se encontra [aqui](https://www.kaggle.com/c/dogs-vs-cats). Dentre essas imagens foram separadas 8000 imagens para treinamento e 2000 imagens para teste.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "wvFs0wxIu7-n"
      },
      "cell_type": "markdown",
      "source": [
        "## Procedimentos Iniciais"
      ]
    },
    {
      "metadata": {
        "id": "Ol_1ulM_u_Ln"
      },
      "cell_type": "markdown",
      "source": [
        "Apagar a pasta catsxdogs caso algum novo dado seja incluído na pasta:"
      ]
    },
    {
      "metadata": {
        "id": "0ybB41aUeRC0"
      },
      "cell_type": "code",
      "source": [
        "#!rm -rf catsxdogs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CwN6J1f0vUxW"
      },
      "cell_type": "markdown",
      "source": [
        "Download da pasta:"
      ]
    },
    {
      "metadata": {
        "id": "u_eqqa_6eZT_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02202b17-a72a-4941-9bae-5eb9e133c64a"
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/cunhamaicon/catsxdogs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'catsxdogs'...\n",
            "remote: Enumerating objects: 10055, done.\u001b[K\n",
            "remote: Total 10055 (delta 0), reused 0 (delta 0), pack-reused 10055 (from 1)\u001b[K\n",
            "Receiving objects: 100% (10055/10055), 301.00 MiB | 10.28 MiB/s, done.\n",
            "Resolving deltas: 100% (19/19), done.\n",
            "Updating files: 100% (10018/10018), done.\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "A3A7Y7GbvY2Y"
      },
      "cell_type": "markdown",
      "source": [
        "Importação dos pacotes:"
      ]
    },
    {
      "metadata": {
        "id": "7q906nBeeHAo"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Dense,GlobalAveragePooling2D\n",
        "from keras.applications import MobileNet\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.mobilenet import preprocess_input\n",
        "# from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Dropout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CbSaiWG3v2kB"
      },
      "cell_type": "markdown",
      "source": [
        "## CNN - Transfer learning:\n",
        "\n",
        "> Adicionar aspas\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ZGFvuzY2v-Sm"
      },
      "cell_type": "markdown",
      "source": [
        "Importando o modelo MobileNet que foi previamente treinado no ImageNet e descartando a última camada de neurônios:"
      ]
    },
    {
      "metadata": {
        "id": "1B3-bKjBwReh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa666588-444c-47b1-bba2-810037d5a540"
      },
      "cell_type": "code",
      "source": [
        "model=MobileNet(weights='imagenet',include_top=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-43901aabfbda>:1: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  model=MobileNet(weights='imagenet',include_top=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n",
            "\u001b[1m17225924/17225924\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "oO7FN2qEwgkf"
      },
      "cell_type": "markdown",
      "source": [
        "Criando a saída do modelo MobileNet:"
      ]
    },
    {
      "metadata": {
        "id": "PwwGWkXWwi9G"
      },
      "cell_type": "code",
      "source": [
        "x=model.output\n",
        "x=GlobalAveragePooling2D()(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m34KAC6_wsPu"
      },
      "cell_type": "markdown",
      "source": [
        "Adicionando uma camada intermediária e a camada final:"
      ]
    },
    {
      "metadata": {
        "id": "f-g5WXZ2ebil"
      },
      "cell_type": "code",
      "source": [
        "x=Dense(50,activation='relu')(x)\n",
        "preds=Dense(1,activation='sigmoid')(x)\n",
        "model=Model(inputs=model.input,outputs=preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "upU4C7VyxJN7"
      },
      "cell_type": "markdown",
      "source": [
        "Visualizando todas as camadas da nova rede criada usando o modelo MobileNetV2:"
      ]
    },
    {
      "metadata": {
        "id": "2VemyqKtfGdM",
        "outputId": "0c78ea76-3a40-4bc3-fbcc-ec43ae3996d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "for i,layer in enumerate(model.layers):\n",
        "  print(i,layer.name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 input_layer\n",
            "1 conv1\n",
            "2 conv1_bn\n",
            "3 conv1_relu\n",
            "4 conv_dw_1\n",
            "5 conv_dw_1_bn\n",
            "6 conv_dw_1_relu\n",
            "7 conv_pw_1\n",
            "8 conv_pw_1_bn\n",
            "9 conv_pw_1_relu\n",
            "10 conv_pad_2\n",
            "11 conv_dw_2\n",
            "12 conv_dw_2_bn\n",
            "13 conv_dw_2_relu\n",
            "14 conv_pw_2\n",
            "15 conv_pw_2_bn\n",
            "16 conv_pw_2_relu\n",
            "17 conv_dw_3\n",
            "18 conv_dw_3_bn\n",
            "19 conv_dw_3_relu\n",
            "20 conv_pw_3\n",
            "21 conv_pw_3_bn\n",
            "22 conv_pw_3_relu\n",
            "23 conv_pad_4\n",
            "24 conv_dw_4\n",
            "25 conv_dw_4_bn\n",
            "26 conv_dw_4_relu\n",
            "27 conv_pw_4\n",
            "28 conv_pw_4_bn\n",
            "29 conv_pw_4_relu\n",
            "30 conv_dw_5\n",
            "31 conv_dw_5_bn\n",
            "32 conv_dw_5_relu\n",
            "33 conv_pw_5\n",
            "34 conv_pw_5_bn\n",
            "35 conv_pw_5_relu\n",
            "36 conv_pad_6\n",
            "37 conv_dw_6\n",
            "38 conv_dw_6_bn\n",
            "39 conv_dw_6_relu\n",
            "40 conv_pw_6\n",
            "41 conv_pw_6_bn\n",
            "42 conv_pw_6_relu\n",
            "43 conv_dw_7\n",
            "44 conv_dw_7_bn\n",
            "45 conv_dw_7_relu\n",
            "46 conv_pw_7\n",
            "47 conv_pw_7_bn\n",
            "48 conv_pw_7_relu\n",
            "49 conv_dw_8\n",
            "50 conv_dw_8_bn\n",
            "51 conv_dw_8_relu\n",
            "52 conv_pw_8\n",
            "53 conv_pw_8_bn\n",
            "54 conv_pw_8_relu\n",
            "55 conv_dw_9\n",
            "56 conv_dw_9_bn\n",
            "57 conv_dw_9_relu\n",
            "58 conv_pw_9\n",
            "59 conv_pw_9_bn\n",
            "60 conv_pw_9_relu\n",
            "61 conv_dw_10\n",
            "62 conv_dw_10_bn\n",
            "63 conv_dw_10_relu\n",
            "64 conv_pw_10\n",
            "65 conv_pw_10_bn\n",
            "66 conv_pw_10_relu\n",
            "67 conv_dw_11\n",
            "68 conv_dw_11_bn\n",
            "69 conv_dw_11_relu\n",
            "70 conv_pw_11\n",
            "71 conv_pw_11_bn\n",
            "72 conv_pw_11_relu\n",
            "73 conv_pad_12\n",
            "74 conv_dw_12\n",
            "75 conv_dw_12_bn\n",
            "76 conv_dw_12_relu\n",
            "77 conv_pw_12\n",
            "78 conv_pw_12_bn\n",
            "79 conv_pw_12_relu\n",
            "80 conv_dw_13\n",
            "81 conv_dw_13_bn\n",
            "82 conv_dw_13_relu\n",
            "83 conv_pw_13\n",
            "84 conv_pw_13_bn\n",
            "85 conv_pw_13_relu\n",
            "86 global_average_pooling2d\n",
            "87 dense\n",
            "88 dense_1\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "MKJNIAKBybNY"
      },
      "cell_type": "markdown",
      "source": [
        "Definindo qual camada da rede será treinada. Nesse caso somente as duas últimas camadas adicionadas:"
      ]
    },
    {
      "metadata": {
        "id": "55aGsU4ufgJE"
      },
      "cell_type": "code",
      "source": [
        "for layer in model.layers[:88]:\n",
        "    layer.trainable=False\n",
        "for layer in model.layers[88:]:\n",
        "    layer.trainable=True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SDq6-50Jy1xv"
      },
      "cell_type": "markdown",
      "source": [
        "## ImageDataGenerator"
      ]
    },
    {
      "metadata": {
        "id": "ofmdrIHVy5j0"
      },
      "cell_type": "markdown",
      "source": [
        "Definindo o tamanho de cada batch:"
      ]
    },
    {
      "metadata": {
        "id": "nYPnSrmQhDSq"
      },
      "cell_type": "code",
      "source": [
        "batch_size = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FoUT3HQZzUm_"
      },
      "cell_type": "markdown",
      "source": [
        "Cada imagem do banco será apresentada a rede de uma forma diferente através do ImageDataGenerator:"
      ]
    },
    {
      "metadata": {
        "id": "aOePa8OAgOaf",
        "outputId": "a9edaec0-6570-4078-9ae1-ee84593f85ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.4,\n",
        "                                   zoom_range = 0.4,\n",
        "                                   height_shift_range=0.3,\n",
        "                                   width_shift_range=0.3,\n",
        "                                   rotation_range=50,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "training_set = train_datagen.flow_from_directory('catsxdogs/training_set',\n",
        "                                                 target_size = (224, 224),\n",
        "                                                 batch_size = batch_size,\n",
        "                                                 class_mode = 'binary')\n",
        "\n",
        "test_set = test_datagen.flow_from_directory('catsxdogs/test_set',\n",
        "                                            target_size = (224, 224),\n",
        "                                            batch_size = batch_size,\n",
        "                                            class_mode = 'binary')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8000 images belonging to 2 classes.\n",
            "Found 2000 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "zN0NGzrc0QMH"
      },
      "cell_type": "markdown",
      "source": [
        "## Treinamento"
      ]
    },
    {
      "metadata": {
        "id": "DlozFnpQ0WW2"
      },
      "cell_type": "markdown",
      "source": [
        "Definindo os parâmetros de compilação da rede:"
      ]
    },
    {
      "metadata": {
        "id": "lpeqcs5hgkd8"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bb1x9_kz0i4P"
      },
      "cell_type": "markdown",
      "source": [
        "Fazendo o treinamento da rede:"
      ]
    },
    {
      "metadata": {
        "id": "3tzvbgsYhkDK",
        "outputId": "40306ebc-b6c5-4c52-bcfe-cbba28c0c2c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(training_set,\n",
        "                   steps_per_epoch=int(8000/batch_size),\n",
        "                   epochs=10,\n",
        "                   validation_data=test_set,\n",
        "                   validation_steps=int(2000/batch_size))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 447ms/step - accuracy: 0.5121 - loss: 0.8243 - val_accuracy: 0.5368 - val_loss: 0.7326\n",
            "Epoch 2/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 414ms/step - accuracy: 0.5178 - loss: 0.7502 - val_accuracy: 0.5958 - val_loss: 0.6668\n",
            "Epoch 3/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 568ms/step - accuracy: 0.5664 - loss: 0.6932 - val_accuracy: 0.6633 - val_loss: 0.6241\n",
            "Epoch 4/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 414ms/step - accuracy: 0.5996 - loss: 0.6641 - val_accuracy: 0.7051 - val_loss: 0.5909\n",
            "Epoch 5/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 412ms/step - accuracy: 0.6294 - loss: 0.6369 - val_accuracy: 0.7399 - val_loss: 0.5630\n",
            "Epoch 6/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 408ms/step - accuracy: 0.6481 - loss: 0.6208 - val_accuracy: 0.7676 - val_loss: 0.5391\n",
            "Epoch 7/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 410ms/step - accuracy: 0.6821 - loss: 0.6009 - val_accuracy: 0.7923 - val_loss: 0.5171\n",
            "Epoch 8/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 408ms/step - accuracy: 0.6896 - loss: 0.5919 - val_accuracy: 0.8075 - val_loss: 0.4955\n",
            "Epoch 9/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 407ms/step - accuracy: 0.7166 - loss: 0.5728 - val_accuracy: 0.8120 - val_loss: 0.4775\n",
            "Epoch 10/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 413ms/step - accuracy: 0.7353 - loss: 0.5555 - val_accuracy: 0.8246 - val_loss: 0.4599\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "bpE6bcT4DgXC"
      },
      "cell_type": "markdown",
      "source": [
        "Salvando o modelo para utilização futura:"
      ]
    },
    {
      "metadata": {
        "id": "9bKzjWBbnXaM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "12f347a1-3c96-48c4-fa60-070c9e32441c"
      },
      "cell_type": "code",
      "source": [
        "model.save('catsxdogs_mobilenet.h5')\n",
        "from google.colab import files\n",
        "files.download('catsxdogs_mobilenet.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c2bd6ef9-fa4d-42a1-924b-3666cc43e874\", \"catsxdogs_mobilenet.h5\", 13416768)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "id": "kBLPwHxpC7MG"
      },
      "cell_type": "markdown",
      "source": [
        "## Previsão"
      ]
    },
    {
      "metadata": {
        "id": "IupvsIC2DHJD"
      },
      "cell_type": "markdown",
      "source": [
        "Mostrando os arquivos da pasta single_prediction com imagens inéditas para a rede classificar:"
      ]
    },
    {
      "metadata": {
        "id": "-zi2res8DTo4",
        "outputId": "97fbf803-20f6-474f-f1a5-00cf68b0672f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "ls catsxdogs/single_prediction"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat_or_dog_1.jpg  cat_or_dog_2.jpg  chino1.jpg  floyd1.jpg  floyd2.jpg  floyd3.jpg  floyd4.jpg\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "CjZPIcKiC-9_"
      },
      "cell_type": "markdown",
      "source": [
        "Escolhendo uma imagem da pasta single_prediction para fazer a previsão:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KVwBeDRtXlvC"
      }
    },
    {
      "metadata": {
        "id": "9qlK2kkPoMD1"
      },
      "cell_type": "code",
      "source": [
        "test_image = image.load_img('catsxdogs/single_prediction/cat_or_dog_1.jpg', target_size = (224, 224))\n",
        "\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis = 0)\n",
        "test_image = test_image/255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lfF9Lf-XoRgH",
        "outputId": "1e3147fe-4e06-4faf-cd44-3c76402670dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "result = model.predict(test_image)\n",
        "\n",
        "if result[0][0] > 0.5:\n",
        "    prediction = 'dog'\n",
        "else:\n",
        "    prediction = 'cat'\n",
        "\n",
        "print(prediction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "dog\n"
          ]
        }
      ]
    }
  ]
}